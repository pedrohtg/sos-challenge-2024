{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and path setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from skimage.filters import gaussian\n",
    "from skimage.measure import label, regionprops, find_contours\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../dataset/street_obstacle_sequences/'\n",
    "raw_path = os.path.join(root, 'raw_data')\n",
    "target_path = os.path.join(root, 'semantic_ood')\n",
    "ood_score_path = os.path.join(root, 'ood_score')\n",
    "ood_prediction_tracked_path = os.path.join(root, 'ood_prediction_tracked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir(raw_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sequences = ['all'] # can be ['all'] to evaluate all sequences or a list with the sequences names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SOS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Obstacle_Sequence_Challenge.datasets.street_obstacle_sequences import StreetObstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_dataset = StreetObstacles(root, sequences=predicted_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Grounding DINO and Segment Anything models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(), \"GroundingDINO\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"segment-anything\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Grounding DINO\n",
    "from GroundingDINO.groundingdino.util import box_ops\n",
    "from GroundingDINO.groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam, SamPredictor\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to download the weights of the models\n",
    "# !mkdir weights\n",
    "# %cd weights\n",
    "\n",
    "# !wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
    "# !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '.'\n",
    "\n",
    "GDINO_CONFIG_PATH = os.path.join(ROOT, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "print(GDINO_CONFIG_PATH, \"; exist:\", os.path.isfile(GDINO_CONFIG_PATH))\n",
    "\n",
    "GDINO_WEIGHTS_NAME = \"groundingdino_swint_ogc.pth\"\n",
    "GDINO_WEIGHTS_PATH = os.path.join(ROOT, \"weights\", GDINO_WEIGHTS_NAME)\n",
    "print(GDINO_WEIGHTS_PATH, \"; exist:\", os.path.isfile(GDINO_WEIGHTS_PATH))\n",
    "\n",
    "SAM_WEIGHTS_NAME = \"sam_vit_h_4b8939.pth\"\n",
    "SAM_WEIGHTS_PATH = os.path.join(ROOT, \"weights\", SAM_WEIGHTS_NAME)\n",
    "print(SAM_WEIGHTS_PATH, \"; exist:\", os.path.isfile(SAM_WEIGHTS_PATH))\n",
    "\n",
    "#\n",
    "# Loading Grounding DINO Model\n",
    "groundingdino_model = load_model(GDINO_CONFIG_PATH, GDINO_WEIGHTS_PATH)\n",
    "\n",
    "#\n",
    "# Loading SAM Model\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# sam_checkpoint = 'sam_vit_h_4b8939.pth'\n",
    "sam = build_sam(checkpoint=SAM_WEIGHTS_PATH)\n",
    "sam.to(device=DEVICE)\n",
    "sam_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_boxes(boxes, phrases):\n",
    "    arr_phr = np.array(phrases)\n",
    "    filtered_ids = set()\n",
    "    \n",
    "    road_ids = set(np.where(arr_phr == 'road')[0])\n",
    "    obj_ids = set(np.where(arr_phr == 'object')[0])\n",
    "    sml_ids = set(np.where(arr_phr == 'small')[0])\n",
    "    sob_ids = set(np.where(arr_phr == 'small object')[0])\n",
    "    \n",
    "    filtered_ids.update(obj_ids)\n",
    "    filtered_ids.update(sml_ids)\n",
    "    filtered_ids.update(sob_ids)\n",
    "    \n",
    "    return list(road_ids) + list(filtered_ids)\n",
    "\n",
    "def get_valid_boxes(image, boxes, logits, iou_threshold=0.9):\n",
    "    H, W, _ = image.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "    iou, _ = box_ops.box_iou(boxes_xyxy, boxes_xyxy)\n",
    "    ioub = iou > iou_threshold\n",
    "    problem_pairs = np.argwhere(np.triu(ioub, k=1))\n",
    "    valid_boxes = set(np.arange(len(boxes_xyxy)).tolist())\n",
    "    supressed_boxes = set()\n",
    "    for boxpair in problem_pairs:\n",
    "        box1, box2 = boxpair[0], boxpair[1]\n",
    "        if logits[box1] >= logits[box2]:\n",
    "            supressed_boxes.add(box2)\n",
    "        else:\n",
    "            supressed_boxes.add(box1)\n",
    "    nonsupressed_boxes = list(valid_boxes-supressed_boxes)\n",
    "    \n",
    "    return nonsupressed_boxes\n",
    "\n",
    "def get_object_boxes(boxes, phrases):\n",
    "    obj_ids = np.where(phrases != 'road')[0]\n",
    "    return boxes[obj_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proposal_masks(masks, scores, logits, phrases, w=(0.7, 0.3)):\n",
    "    H, W = masks.shape[-2:]\n",
    "    \n",
    "    ood_score_mask = np.full((H, W), 0, dtype=np.float32)\n",
    "    ood_mask = np.full((H, W), 255, dtype=np.uint8)\n",
    "    road_mask = np.full((H, W), False, dtype=np.uint8)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))\n",
    "    kernel_medium = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(20,20))\n",
    "    kernel_road = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(50,50))\n",
    "    \n",
    "    for i, label in enumerate(phrases):\n",
    "        mask = masks[i][0].cpu().numpy().astype(np.uint8)\n",
    "                \n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel_medium)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel).astype(bool)\n",
    "        \n",
    "        if label == 'road':\n",
    "            mask = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel_road)\n",
    "            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel_road).astype(bool)\n",
    "            \n",
    "            ood_mask[mask] = 0\n",
    "        else:\n",
    "            ood_mask[mask] = 254# scores[i].item() \n",
    "            ood_score_mask[mask] = w[0]*logits[i].item() + w[1]*scores[i].item() # scores[i].item()\n",
    "        \n",
    "    return ood_mask, ood_score_mask\n",
    "\n",
    "def get_proposal_regions(mask):\n",
    "    # road: mask == 0, ood obj: == 254 for , background: == 255\n",
    "    obj_label_img = label(mask == 254)\n",
    "    obj_regions = regionprops(obj_label_img)\n",
    "    \n",
    "    road_label_img = label(mask == 0)\n",
    "    road_regions = regionprops(road_label_img)\n",
    "    \n",
    "    return obj_label_img, obj_regions, road_label_img, road_regions\n",
    "\n",
    "def get_ood_regions(obj_label_img, obj_regions, road_label_img, road_regions, inter_th=0.15):\n",
    "    \n",
    "    selected_obj_ids = []\n",
    "    selected_obj_bbxs = []\n",
    "    selected_obj_instance_lbls = []\n",
    "    \n",
    "    for i, obj in enumerate(obj_regions):\n",
    "        \n",
    "        obj_ch = np.zeros_like(obj_label_img, dtype=np.uint8)\n",
    "        minr, minc, maxr, maxc = obj.bbox\n",
    "        obj_ch[minr:maxr, minc:maxc] = obj.image_convex\n",
    "        \n",
    "        obj_px_area = np.count_nonzero(obj_ch)\n",
    "        \n",
    "        for j, road in enumerate(road_regions):\n",
    "            \n",
    "            if i in selected_obj_ids: break\n",
    "            \n",
    "            road_ch = np.zeros_like(obj_label_img, dtype=np.uint8)\n",
    "            rd_minr, rd_minc, rd_maxr, rd_maxc = road.bbox\n",
    "            road_ch[rd_minr:rd_maxr, rd_minc:rd_maxc] = road.image_convex\n",
    "            \n",
    "            road_px_area = np.count_nonzero(road_ch)\n",
    "            min_px_area = min(road_px_area, obj_px_area)\n",
    "            \n",
    "            intersection = obj_ch & road_ch\n",
    "            \n",
    "            intersection_px_area = np.count_nonzero(intersection)\n",
    "            \n",
    "            if intersection_px_area / min_px_area >= inter_th:\n",
    "                selected_obj_ids.append(i)\n",
    "                selected_obj_bbxs.append([minc, minr, maxc, maxr]) #lt, rb\n",
    "                selected_obj_instance_lbls.append(obj.label)\n",
    "    \n",
    "    return selected_obj_ids, selected_obj_bbxs, selected_obj_instance_lbls   \n",
    "    \n",
    "\n",
    "def get_filtered_proposal_mask(proposal_mask, obj_label_img, obj_instance_lbls, road_label_img):\n",
    "    mask = np.full_like(proposal_mask, 255, dtype=np.uint8)\n",
    "    \n",
    "    # fill roads\n",
    "    mask[road_label_img > 0] = 0\n",
    "    \n",
    "    # fill objects\n",
    "    for lbl in obj_instance_lbls:\n",
    "        mask[obj_label_img == lbl] = 254\n",
    "        \n",
    "    return mask\n",
    "    \n",
    "def get_ood_map(mask, score_mask):\n",
    "    H, W = mask.shape[-2:]\n",
    "    ood_mask = np.zeros((H, W), dtype=np.float32)\n",
    "    ood_mask[mask == 254] = score_mask[mask == 254]\n",
    "    \n",
    "    return ood_mask\n",
    "\n",
    "def get_track_map(masks, ids, mask_id):\n",
    "    H, W = masks.shape[-2:]\n",
    "    track_mask = np.full((H, W), 255, dtype=np.uint8)\n",
    "    \n",
    "    for i, label in enumerate(ids):\n",
    "        mask = masks == mask_id[i] \n",
    "        # 254 # id == road: value = 0, ood object == not road: value 254\n",
    "        if label == 0: continue \n",
    "        else:\n",
    "            track_mask[mask] = label\n",
    "        \n",
    "    return track_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greed algorithm for tracking. Match boxes in the actual frame with boxes from previous frame with higher iou\n",
    "def match_boxes(cur_box, prev_box, track_ids, iou_threshold=0.1):\n",
    "    cur_box = torch.Tensor(cur_box)\n",
    "    prev_box = torch.Tensor(prev_box)\n",
    "    \n",
    "    iou, _ = box_ops.box_iou(cur_box, prev_box)\n",
    "\n",
    "    temp_track_ids = np.zeros(len(cur_box), dtype=np.uint8)\n",
    "\n",
    "    for bi in range(len(cur_box)):\n",
    "        if iou[bi].max() >= iou_threshold:\n",
    "            matched_box = np.argmax(iou[bi])\n",
    "            temp_track_ids[bi] = track_ids[matched_box]\n",
    "\n",
    "    temp_track_ids[temp_track_ids == 0] = np.arange(len(temp_track_ids[temp_track_ids == 0])) + np.max(temp_track_ids) + 1\n",
    "\n",
    "    return temp_track_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run the detection of objects and road\n",
    "TEXT_PROMPT = \"small object in the road, small object, road\"\n",
    "BOX_TRESHOLD = 0.15\n",
    "TEXT_TRESHOLD = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the dataset\n",
    "\n",
    "- The GroundDINO detects objects and the road (following the `TEXT_PROMPT`).\n",
    "- The detections are filtered, to remove know-objects (if they are in the `TEXT_PROMPT`) and to remove possible duplicates (boxes with high intersections, only the most confident is kept), and lastly reorder the boxes to position 'road' boxes first on the list.\n",
    "- The boxes are passed, along the image, as input to the SAM model that provides a segmentation for each box.\n",
    "- These segmentations are then labeled (and have a ood score attached if they are objects), using refined masks (through morphological operations)\n",
    "- For each object segmentation, its convex hull is computed and only objects with some intersection with the road mask are mantained.\n",
    "- The remainder objects are then allotted a track id. Tracking through frames is done using a greedy algorithm that matches boxes from one frame with boxes with the highest intersection in the next frame.\n",
    "- Then both the track_id and ood_score maps are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path in sos_dataset.images:\n",
    "    print(f'... processing sequence {img_path.split(\"/\")[-2]}, frame {img_path.split(\"/\")[-1]}')\n",
    "    # Load image\n",
    "    image_source, image = load_image(img_path)\n",
    "    \n",
    "    # Get predictions\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=groundingdino_model,\n",
    "        image=image,\n",
    "        caption=TEXT_PROMPT,\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # Remove prepositions and articles from the text labels\n",
    "    phrases = [x.replace('the ', '').replace('in ', '') for x in phrases]\n",
    "    \n",
    "    # Filter id bounboxes (only return road and small object boxes)\n",
    "    filtered_boxes = filter_boxes(boxes, phrases)\n",
    "    boxes = boxes[filtered_boxes]\n",
    "    logits = logits[filtered_boxes]\n",
    "    phrases= np.array(phrases)[filtered_boxes]    \n",
    "        \n",
    "    # Filter overlapping boundboxes\n",
    "    valid_boxes = get_valid_boxes(image_source, boxes, logits)\n",
    "    boxes = boxes[valid_boxes]\n",
    "    logits = logits[valid_boxes]\n",
    "    phrases= phrases[valid_boxes]\n",
    "    \n",
    "    # Get Segmentation\n",
    "    sam_predictor.set_image(image_source)\n",
    "\n",
    "    # box: normalized box xywh -> unnormalized xyxy\n",
    "    H, W, _ = image_source.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "    transformed_boxes = sam_predictor.transform.apply_boxes_torch(boxes_xyxy, image_source.shape[:2]).to(DEVICE)\n",
    "    masks, scores, _ = sam_predictor.predict_torch(\n",
    "                point_coords = None,\n",
    "                point_labels = None,\n",
    "                boxes = transformed_boxes,\n",
    "                multimask_output = False,\n",
    "            )\n",
    "    # get proposal masks\n",
    "    proposal_mask, score_mask = get_proposal_masks(masks, scores, logits, phrases)\n",
    "    # get regions of interest\n",
    "    obj_label_img, obj_regions, road_label_img, road_regions = get_proposal_regions(proposal_mask)\n",
    "    # filter objects without intersection with the road mask\n",
    "    selected_obj_ids, selected_obj_bbxs, selected_obj_instance_lbls  = get_ood_regions(obj_label_img, \n",
    "                                                                                       obj_regions, \n",
    "                                                                                       road_label_img, \n",
    "                                                                                       road_regions)\n",
    "    # get the filtered version of the labeled map\n",
    "    filtered_proposal_mask = get_filtered_proposal_mask(proposal_mask, \n",
    "                                                        obj_label_img, selected_obj_instance_lbls,\n",
    "                                                        road_label_img)\n",
    "    \n",
    "    # OOD score\n",
    "    ood_score = get_ood_map(filtered_proposal_mask, score_mask)           # get_ood_map(masks, scores, phrases)\n",
    "    # ood_score = gaussian(ood_score, sigma=3)                            # smooth ood score predictions \n",
    "    \n",
    "    \n",
    "    # Get object boxes\n",
    "    obj_boxes = selected_obj_bbxs                                         # get_object_boxes(boxes, phrases)\n",
    "    \n",
    "    #######################\n",
    "    # Tracking Process ####\n",
    "    \n",
    "    cur_frame = img_path.split(\"/\")[-1].split('_')[0]\n",
    "\n",
    "    if cur_frame == '000000': # first frame\n",
    "        last_tracked_obj_id = 0 \n",
    "        prev_obj_boxes = []\n",
    "        \n",
    "    if len(obj_boxes) < 1:\n",
    "        cur_track_ids = []\n",
    "    else:\n",
    "        if len(prev_obj_boxes) > 0:\n",
    "            cur_track_ids = np.zeros(len(obj_boxes), dtype=np.uint8)\n",
    "            cur_track_ids = match_boxes(obj_boxes, prev_obj_boxes, prev_track_ids)\n",
    "        else:\n",
    "            cur_track_ids = np.arange(len(obj_boxes), dtype=np.uint8) + last_tracked_obj_id + 1\n",
    "                \n",
    "        last_tracked_obj_id = max(last_tracked_obj_id, np.max(cur_track_ids))\n",
    "\n",
    "    prev_track_ids = cur_track_ids\n",
    "    prev_obj_boxes = obj_boxes\n",
    "\n",
    "    padded_track_ids = np.insert(cur_track_ids, 0, np.where(phrases == 'road')[0]) # Pad the track ids with zeros for road box\n",
    "\n",
    "    # Tracking labels\n",
    "    ood_track = get_track_map(obj_label_img, cur_track_ids, selected_obj_instance_lbls)\n",
    "    \n",
    "    #######################\n",
    "    #######################\n",
    "    \n",
    "    \n",
    "    # Save OOD score/ tracking\n",
    "    actual_seq = img_path.split('/')[-2]\n",
    "    actual_frame = img_path.split('/')[-1].replace('_raw_data.jpg', '.npy')\n",
    "    actual_frame_label = img_path.split('/')[-1].replace('_raw_data.jpg', '_label.npy')\n",
    "    \n",
    "    # Tracking save\n",
    "    os.makedirs(os.path.join(ood_prediction_tracked_path, actual_seq), exist_ok=True)\n",
    "    \n",
    "    ood_track_path = os.path.join(ood_prediction_tracked_path, actual_seq, actual_frame)\n",
    "    np.save(ood_track_path, ood_track)\n",
    "    \n",
    "    # OOD save\n",
    "    os.makedirs(os.path.join(ood_score_path, actual_seq), exist_ok=True)\n",
    "    \n",
    "    ood_map_path = os.path.join(ood_score_path, actual_seq, actual_frame)\n",
    "    ood_lbl_path = os.path.join(ood_score_path, actual_seq, actual_frame_label)\n",
    "    np.save(ood_map_path, ood_score)\n",
    "    np.save(ood_lbl_path, filtered_proposal_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For result visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sequence, frame=0, gray=False, pred=False):\n",
    "    sequence_r_path = os.path.join(raw_path, f'sequence_{sequence}')\n",
    "    sequence_t_path = os.path.join(target_path, f'sequence_{sequence}')\n",
    "    sequence_p_path = os.path.join(ood_score_path, f'sequence_{sequence}')\n",
    "    \n",
    "    frame = f'{frame:06d}'\n",
    "    img_path = os.path.join(sequence_r_path, f'{frame}_raw_data.jpg')\n",
    "    tgt_path = os.path.join(sequence_t_path, f'{frame}_semantic_ood.png')\n",
    "    prd_path = os.path.join(sequence_p_path, f'{frame}_label.npy')\n",
    "    scr_path = os.path.join(sequence_p_path, f'{frame}.npy')\n",
    "    \n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    target = Image.open(tgt_path).convert(\"I;16\" if gray else \"RGB\")\n",
    "    prd = None\n",
    "    \n",
    "    if not pred:\n",
    "        return image, target\n",
    "    else:\n",
    "        prd = np.load(prd_path)\n",
    "        scr = np.load(scr_path)\n",
    "        return image, target, prd, scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame has to be a multiple of 8\n",
    "def show_gt_pair(sequence, frame=0, gray=False):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(24, 8))\n",
    "    \n",
    "    image, target = get_data(sequence, frame, gray=gray)\n",
    "    \n",
    "    ax[0].imshow(image)\n",
    "    ax[1].imshow(target)\n",
    "    \n",
    "    ax[0].axis('off')\n",
    "    ax[1].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pair(im1, im2):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(24, 8))\n",
    "    \n",
    "    ax[0].imshow(im1)\n",
    "    ax[1].imshow(im2)\n",
    "    \n",
    "    ax[0].axis('off')\n",
    "    ax[1].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gray(npimg):\n",
    "    npimg[npimg == 0] = 63\n",
    "    npimg[npimg == 255] = 0\n",
    "    npimg[npimg == 254] = 255\n",
    "    \n",
    "    return npimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gt_pred(sequence, frame=0, gray=False):\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(20, 10))\n",
    "    \n",
    "    image, target, pred, score = get_data(sequence, frame, gray=gray, pred=True)\n",
    "    \n",
    "    if gray:\n",
    "        target = Image.fromarray(to_gray(np.array(target)))\n",
    "        pred = Image.fromarray(to_gray(pred))\n",
    "    else:\n",
    "        pred = Image.fromarray(pred)\n",
    "    \n",
    "    ax[0][0].imshow(image)\n",
    "    ax[0][1].imshow(score)\n",
    "    ax[1][0].imshow(target)\n",
    "    ax[1][1].imshow(pred)\n",
    "    \n",
    "    ax[0][0].axis('off')\n",
    "    ax[0][1].axis('off')\n",
    "    ax[1][0].axis('off')\n",
    "    ax[1][1].axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return pred, score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
